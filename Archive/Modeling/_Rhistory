library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
summary(Wage)
library(Hmisc)
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
?plot
str(training)
library(ggplot2)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
library(Hmisc)
library(GGally)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
featurePlot(x=training, y=training$CompressiveStrength, plot="pairs")
qplot(data = training, y = CompressiveStrength)
str(training)
attach(training)
Cementcut <- cut2(Cement, g=4)
BlastFurnaceSlagCut <- cut2(BlastFurnaceSlag, g=4)
Cementcut <- cut2(Cement, g=4)
BlastFurnaceSlagCut <- cut2(BlastFurnaceSlag, g=4)
FlyAshCut <- cut2(FlyAsh, g=4)
qplot(data = training, y = CompressiveStrength, colour = FlyAshCut)
qplot(data = training, y = CompressiveStrength, colour = BlastFurnaceSlagCut)
qplot(data = training, y = CompressiveStrength, colour = Cementcut)
SuperplasticizerCut <- cut2(Superplasticizer, g=4)
qplot(data = training, y = CompressiveStrength, colour = FlyAshCut)
qplot(data = training, y = CompressiveStrength, colour = WaterCut)
WaterCut <- cut2(Water, g=4)
SuperplasticizerCut <- cut2(Superplasticizer, g=4)
qplot(data = training, y = CompressiveStrength, colour = WaterCut)
qplot(data = training, y = CompressiveStrength, colour = SuperplasticizerCut)
FineAggregateCut <- cut2(FineAggregate, g=4)
qplot(data = training, y = CompressiveStrength, colour = AgeCut)
AgeCut <- cut2(Age, g=4)
FineAggregateCut <- cut2(FineAggregate, g=4)
qplot(data = training, y = CompressiveStrength, colour = AgeCut)
qplot(data = training, y = CompressiveStrength, colour = FineAggregateCut)
qplot(data=training,y = CompressiveStrength, x= FlyAsh)
qplot(SuperPlasticizer)
qplot(SuperPlasticizer, data=training)
qplot(training$SuperPlasticizer)
?qplot
qplot(x = SuperPlasticizer, data=training)
str(training)
qplot(x = Superplasticizer, data=training)
log(0)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training)
nm <- colnames(training)
?substr
substr("abcde", 1, 3)
use_nm <- nm[substr(nm, 1, 2) == "IL"]
use_nm
training_IL <- training[, substr(nm, 1, 2) == "IL"]
str(training_IL)
?svd
t <- svd(training_IL)
str(t)
t$d
plot(t$d^2/sum(t$d^2), type="b")
t$d^2/sum(t$d^2)
round(t$d^2/sum(t$d^2),1)
round(t$d^2/sum(t$d^2),2)
round(t$d/sum(t$d),2)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training)
nm <- colnames(training)
use_nm <- nm[substr(nm, 1, 2) == "IL"]
training_IL <- training[, substr(nm, 1, 2) == "IL"]
str(training_IL)
training_IL_scale <- preProcess(training_IL, method = c("center", "scale"))
t <- svd(training_IL_scale)
str(training_IL_scale)
training_IL_scale <- scale(training_IL, center = TRUE, scale = TRUE)
str(training_IL_scale)
str(training_IL)
training_IL_scale <- scale(training_IL, center = TRUE, scale = TRUE)
str(training_IL_scale)
preProc <- preProcess(training_IL, method=c("center", "scale"))
df <- predict(preProc, training_IL)
str(df)
t <- svd(df)
str(t)
t$d
plot(t$d^2/sum(t$d^2), type="b")
round(t$d/sum(t$d),2)
v <- round(t$d/sum(t$d),2)
sum(v[1:5])
sum(v[1:7])
sum(v[1:9])
sum(v[1:8])
str(training)
training2 <- cbind(training_IL, training$diagnosis)
str(training2)
colnames(training2)[13] <- "diagnosis"
str(training2)
?train
modelFit <- train(training2$diagnosis ~ ., method = "glm", preProcess = c("pca", pcaComp = 9))
modelFit <- train(training2$diagnosis ~ ., method = "glm", preProcess = c("pca", pcaComp = 9), data=training)
preProc <- preProcess(training_IL, method="pca", pcaComp = 9)
trainPC <- predict(preProc, training_IL)
modelFit <- train(training2$diagnosis~., method="glm", data=trainPC)
testing_IL <- testing[, substr(nm, 1, 2) == "IL"]
testing_IL <- testing[, substr(nm, 1, 2) == "IL"]
testing2 <- cbind(testing_IL, testing$diagnosis)
colnames(testing2)[13] <- "diagnosis"
testPC <- predict(preProc, testing_IL)
confusionMatrix(testing2$diagnosis, predict(modelFit, testPC))
modelFit2 <- train(training2$diagnosis ~ ., method="glm", data = training2)
summary(modelFit2)
confusionMatrix(testing2$diagnosis, predict(modelFit2, testing_IL))
modelFit2 <- train(training2$diagnosis ~ ., method="glm", data = training_IL)
confusionMatrix(testing2$diagnosis, predict(modelFit2, testing_IL))
training_IL <- training[, substr(nm, 1, 2) == "IL"]
pre <- preProcess(training_IL, method = c("center, scale"))
pre <- preProcess(training_IL, method = c("center", "scale"))
df <- predict(pre, training_IL)
t <- svd(df)$d
t
l <- t^2 / sum(t^2)
l
sum(l[1:8])
sum(l[1:7])
sum(l[1:6])
preProc <- preProcess(training_IL, method="pca", pcaComp = 7)
trainPC <- predict(preProc, training_IL)
modelFit <- train(training2$diagnosis~., method="glm", data=trainPC)
testPC <- predict(preProc, testing_IL)
confusionMatrix(testing2$diagnosis, predict(modelFit, testPC))
library(quantmod)
install.packages("quantmod")
library(quantmod)
from.dat <- as.Date("01/01/08", format = "%m/%d/%y")
to.dat <- as.Date("12/31/12", format = "%m/%d/%y")
getSymbols("GOOG", src = "google", from = from.dat, to = to.dat)
?getSymbols
mGoog <- to.monthly(GOOG)
googOpen <- Op(mGoog)
library(quantmod)
from.dat <- as.Date("01/01/08", format = "%m/%d/%y")
to.dat <- as.Date("12/31/12", format = "%m/%d/%y")
getSymbols("GOOG", src = "google", from = from.dat, to = to.dat)
mGoog <- to.monthly(GOOG)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
eset.seed(125)
set.seed(125)
str(segmentationOriginal)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
fit <- train(Case ~., data = training, method = "rpart")
library(rattle)
fit
summary(fit)
fancyRpartPlot(fit)
str(fit)
fit$finalModel
fancyRpartPlot(fit$finalModel)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list=FALSE)
str
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
str(training)
training <- subset(segmentationOriginal, Case == "Train")
testing <- subset(segmentationOriginal, Case == "Test")
fit <- train(Class ~., data = training, method = "rpart")
fancyRpartPlot(fit$finalModel)
install.packages("pgmm")
data(olive)
library(pgmm)
data(olive)
olive = olive[,-1]
str(olive)
fit <- train(Area ~., data=olive, method = "rpart")
out <- predict(fit, newdata = as.data.frame(t(colMeans(olive))))
out
fit <- train(Area ~., data=olive, method = "tree")
fit <- train(Area ~., data=olive, method = "rpart")
out <- predict(fit, newdata = as.data.frame(t(colMeans(olive))))
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
str(trainSA)
fit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = trainSA,
method = "glm", family = "binomial")
summary(fit)
summary(fit$finalModel)
fit
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd, predict(fit, trainSA))
missClass(testSA$chd, predict(fit, testSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
?transform
str(vowel.train)
transform(vowel.train, y = as.factor(y))
transform(vowel.test, y = as.factor(y))
set.seed(33833)
?varImp
?train
fit <- train(y~., data = vowel.train, method="rf")
varImp(fit$finalModel )
varImp(fit$finalModel)
str(fit)
str(fit$finalModel)
?varImp
varImp(fit)
varImp(fit$finalModel)
fit <- train(y~., data = vowel.train, method="rf")
t <- fit$finalModel
varImp(t)
varImp(t, scale = FALSE)
fit <- train(y~., data = vowel.train, method="rf", importance = TRUE)
t <- fit$finalModel
varImp(t, scale = FALSE)
o <- varImp(t, scale = FALSE)
o[order(-o)]
o[-order(o)]
o[order(o)]
str(o)
o <- varImp(t, scale = FALSE)$Overall
str(o)
o
o <- varImp(t, scale = FALSE)
str(o)
o
o[order(o$Overall)]
o[order(o$Overall),]
library(caret)
adData = data.frame(diagnosis,predictors)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
str(training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
library(Hmisc)
str(training)
y <- cut2(x=training$Cement, g=3)
str(y)
?qplot
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength,
colour = cut2(x=training$Cement, g=3))
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength,
colour = cut2(x=training$BlastFurnaceSlag, g=3))
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength,
colour = cut2(x=training$FlyAsh, g=3))
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(training$SuperPlasticizer)
str(training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(training$SuperPlasticizer)
qplot(training$SuperPlasticizer, geom = hist)
qplot(x=training$SuperPlasticizer)
training$SuperPlasticizer
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
str(inTrain)
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
qplot(x=training$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training)
substr("abcdef", 2, 4)
substr("abcdef", 1, 2)
idx <- (substring(colnames(training),1,2) == "IL")
idx
training_il <- training[, idx]
training_il <- training[, c(1, idx)]
str(training_il)
b <- c(1, idx)
b <- c(1, idx)
str(b)
idx[1] <- TRUE
training_il <- training[, idx]
str(training_il)
idx <- (substring(colnames(training),1,2) == "IL")
training_il <- training[, idx]
training_il_z <- scale(training_il)
m <- svd(training_il_z)
str(m)
m$d
a <- m$d
b <- sum(a * a)
e <- a*a
sum(e[1:7])/b
sum(e[1:8])/b
sum(e[1:9])/b
preProc <- proProcess(training_il, method = "pca", pcaComp = 9)
preProc <- preProcess(training_il, method = "pca", pcaComp = 9)
trainPC <- predict(preProc, training_il)
fit_pca <- train(training$diagnosis ~ ., data = trainPC, method = "glm")
fit <- train(training$diagnosis ~ ., data = training_il, method = "glm")
mean(testing$diagnosis == predict(fit_pca, testing))
f <- predict(fit_pca, testing)
testing_il <- testing[, idx]
testPC <- predict(preProc, testing_il)
mean(testing$diagnosis == predict(fit_pca, testPC))
mean(testing$diagnosis == predict(fit, testing))
mean(testing$diagnosis == predict(fit, testing_il))
fit <- train(training$diagnosis ~ ., data = training_il, method = "glm")
testing_il <- testing[, idx]
testPC <- predict(preProc, testing_il)
mean(testing$diagnosis == predict(fit_pca, testPC))
mean(testing$diagnosis == predict(fit, testing_il))
confusionMatrix(testing$diagnosis, predict(fit, testing_il))
sum(e[1:9])/b
sum(e[1:7])/b
sum(e[1:6])/b
preProc <- preProcess(training_il, method = "pca", pcaComp = 7)
trainPC <- predict(preProc, training_il)
fit_pca <- train(training$diagnosis ~ ., data = trainPC, method = "glm")
fit <- train(training$diagnosis ~ ., data = training_il, method = "glm")
testing_il <- testing[, idx]
testPC <- predict(preProc, testing_il)
mean(testing$diagnosis == predict(fit_pca, testPC))
confusionMatrix(testing$diagnosis, predict(fit, testing_il))
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
set.seed(125)
str(segmentationOriginal)
inTrain = createDataPartition(segmentationOriginal$Case, p = 3/4, list = FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
str(training)
str(testing)
fit <- traing(Case~., data = training, method = "CART")
fit <- train(Case~., data = training, method = "CART")
fit <- train(Case~., data = training, method = "rpart")
library(rattle)
fancyRpartPlot(fit)
str(fit)
fancyRpartPlot(fit)
str(training)
training <- segmentationOriginal[segmentationOriginal$Case == "Train",]
testing <- segmentationOriginal[segmentationOriginal$Case == "Test",]
str(training)
str(testing)
fit <- train(Class~., data = training, method = "rpart")
library(caret)
fancyRpartPlot(fit)
fancyRpartPlot(fit$finalModel)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
str(trainSA)
fit4 <- train(chd ~ age + alcohol + obesity + tabacco + typea + ldl,
method = "glm", family = "binomial")
fit4 <- train(chd ~ age + alcohol + obesity + tabacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
fit4 <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, predict(fit4, testSA))
missClass(trainSA$chd, predict(fit4, trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
?transform
transform(vowel.train, y = as.factor(y))
transform(vowel.test, y = as.factor(y))
str(vowel.train)
transform(vowel.train, y = as.factor(y))
transform(vowel.test, y = as.factor(y))
str(vowel.train)
transform(vowel.train, y = as.character(y))
str(vowel.train)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
str(vowel.train)
set.seed(33833)
?varimp
varlmp
?varlmp
?varImp
fit5 <- train(y~., data = vowel.train, method = "rf", prox = TRUE, importance = TRUE)
v <- varImp(fit5$finalModel)
str(v)
v
varImp(fit5$finalModel)
varImp(fit5)
VarImp(fit5)
fit5 <- train(y~., data = vowel.train, method = "rf",  importance = TRUE)
varImp(fit5$finalModel)
str(model2$finalModel)
fit5
?varImp
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
fit5 <- train(y~., data = vowel.train, method = "rf",  importance = TRUE)
varImp(fit5$finalModel)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
str(vowel.train)
fit5 <- train(y~., data = vowel.train, method = "rf",  importance = TRUE)
varImp(fit5$finalModel)
varImp(fit5$finalModel, useModel = FALSE)
varImp(fit5)
setwd("~/GoogleDrive/Funda/Modeling")
library(caret)
library(rattle)
#Load data file
input <- read.csv("fundaInventory_20150509.csv", sep = "|")
str(input)
input2 <- input[input$price <= 5000,]    #Filter out cases where we crawled (incorrectly) the sale price
input2$exterior_space[is.na(input2$exterior_space)] <- 0 #Exterior space = NA -> exterior space = 0
funda <- input2
set.seed(1234)
inTrain <- createDataPartition(funda$price, p=0.75, list = FALSE)
training <- funda[inTrain,]
testing <- funda[-inTrain,]
fit1 <- train(price ~ living_area + exterior_space + hood_avg_property_value +
hood + height + number_rooms + year_of_construction + type_of_property,
data = training, method = "glm")    #Fit a glm model
fit2 <- train(price ~ living_area + exterior_space + hood_avg_property_value +
hood + height + number_rooms + year_of_construction + type_of_property,
data = training, method = "enet")    #Fit a elasticNet model
fit3 <- train(price ~ living_area + exterior_space + hood_avg_property_value +
hood + height + number_rooms + year_of_construction + type_of_property,
data = training, method = "lasso")    #Fit a lasso
testing.complete <- testing[complete.cases(testing),]
testing.complete$glm.pred <- predict(fit1, newdata = testing.complete)
testing.complete$glm.diff <- testing.complete$glm.pred - testing.complete$price
testing.complete$glm.percent <- abs(testing.complete$glm.pred / testing.complete$price -1)
mean(testing.complete$glm.percent)
write.csv(testing.complete, file = "glm_testing_result.csv", row.names =  FALSE)
